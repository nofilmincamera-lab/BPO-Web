# Canonical Paths - BPO Intelligence Pipeline

**Single Source of Truth for Dataset Paths and Production Scripts**

**Last Updated:** October 31, 2025

---

## Production Dataset Paths

### Raw Dataset (Input)
```
data/raw/dataset_webscrape-bpo_2025-10-13_10-15-17-310 (1).json
```
- **Size:** 854MB
- **Format:** JSON (single array)
- **Documents:** 45,403
- **Source:** Web scraping (October 13, 2025)
- **Status:** ✅ Canonical production dataset

### Preprocessed Dataset (Production Use)
```
data/processed/preprocessed.jsonl
```
- **Format:** JSON Lines (one document per line)
- **Status:** ✅ Canonical preprocessed dataset for all extraction scripts
- **Generated By:** `scripts/preprocess.py`
- **Features:**
  - Streaming processed (no memory overload)
  - Deduplicated via Bloom filters
  - URL canonicalization applied
  - Proper text extraction and cleaning

---

## ❌ FORBIDDEN Datasets

**DO NOT USE** these datasets - they are deprecated, failed conversions, or test-only:

### Failed Conversions
- `data/preprocessed/dataset_45000_converted.jsonl` - **FAILED CONVERSION**
  - Created by deprecated `convert_raw_dataset.py`
  - Loads entire file to memory
  - No deduplication
  - Incompatible with production pipeline

### Legacy Datasets
- `data/preprocessed/preprocessed_full.jsonl` - **LEGACY/TEST ONLY**
- `data/preprocessed/bpo_preprocessed_full.jsonl` - **LEGACY/TEST ONLY**

### Test Fixtures
- `data/preprocessed/test_5000_rich.jsonl` - Test only (5,000 docs)
- `data/preprocessed/test_*.jsonl` - Test fixtures
- `data/test_10.jsonl` - Minimal test set (10 docs)

---

## Production Scripts

### Data Preprocessing

**Canonical Preprocessor:**
```bash
python scripts/preprocess.py \
  --input "data/raw/dataset_webscrape-bpo_2025-10-13_10-15-17-310 (1).json" \
  --output "data/processed/preprocessed.jsonl"
```

**Features:**
- Streaming JSON parser (ijson) - handles 850MB+ files
- Bloom filter deduplication
- URL canonicalization
- Text extraction and cleaning
- Memory efficient

---

### Entity Extraction

#### Option 1: Orchestrated with Prefect (Recommended)
```bash
python queue_extraction_prefect.py
```

**Features:**
- Uses `src/flows/extraction_flow.py`
- Prefect orchestration (retry, caching, monitoring)
- Heuristics-first multi-tier pipeline
- Monitor at http://localhost:4200

**Pipeline:**
- Tier 1: Heuristics (confidence: 0.90)
- Tier 2: Regex patterns (confidence: 0.92)
- Tier 3: spaCy NER (confidence: 0.70-0.85)

#### Option 2: Direct Execution (No Prefect)
```bash
python run_direct_extraction.py
```

**Features:**
- Uses `src/flows/extraction_flow.py` functions directly
- Same heuristics-first multi-tier pipeline
- No Prefect overhead
- Direct database writes

#### Option 3: Simplified Extraction (Testing/Development)
```bash
python run_simple_extraction.py --source /data/processed/preprocessed.jsonl
```

**Features:**
- Basic regex extraction (NOT full heuristics pipeline)
- Minimal dependencies
- Good for testing/development
- **NOT recommended for production**

---

## ❌ Deprecated Scripts (Archived)

### Archived Scripts - DO NOT USE

**Moved to `archive/` directory:**

1. **`archive/run_gpu_extraction_deprecated.py`**
   - Bypasses heuristics pipeline
   - Uses vanilla spaCy only
   - Fixed confidence (0.8)
   - Loads entire corpus to memory (OOM risk)

2. **`archive/convert_raw_dataset_deprecated.py`**
   - Insufficient preprocessing
   - No deduplication
   - No URL canonicalization
   - Creates forbidden `dataset_45000_converted.jsonl`

**See:** `archive/README_DEPRECATED_SCRIPTS.md` for full details

---

## Directory Structure

```
BPO-Web/
├── data/
│   ├── raw/
│   │   └── dataset_webscrape-bpo_2025-10-13_10-15-17-310 (1).json  ← CANONICAL RAW
│   ├── processed/
│   │   └── preprocessed.jsonl  ← CANONICAL PREPROCESSED
│   └── preprocessed/
│       └── test_*.jsonl  ← TEST FIXTURES ONLY
│
├── scripts/
│   └── preprocess.py  ← CANONICAL PREPROCESSOR
│
├── archive/
│   ├── run_gpu_extraction_deprecated.py  ← DEPRECATED
│   ├── convert_raw_dataset_deprecated.py  ← DEPRECATED
│   └── README_DEPRECATED_SCRIPTS.md
│
├── Production Scripts (use these):
│   ├── queue_extraction_prefect.py  ← RECOMMENDED (Prefect)
│   ├── run_direct_extraction.py  ← RECOMMENDED (Direct)
│   ├── run_extraction.py
│   ├── run_standalone_extraction.py
│   └── run_simple_extraction.py  ← Testing/Dev only
│
└── src/
    └── flows/
        └── extraction_flow.py  ← Multi-tier extraction pipeline
```

---

## Heuristics Data

**Location:** `Heuristics/`

### Core Files
- `ner_relationships.json` - Main heuristics (129 providers, 940 products)
- `company_aliases_clean.json` - Company name normalization (3,925 aliases)
- `products.json` - Product taxonomy
- `tech_terms.json` - Technology terms
- `countries.json` - Geographic entities
- `partnerships.json` - Provider-partner relationships
- `content_types.json` - Content categorization
- `taxonomy_*.json` - Hierarchical taxonomy
- `version.json` - Heuristics version tracking

**Current Version:** 2.0.0

---

## Database

### PostgreSQL (Main Database)
- **Host:** `postgres` (Docker) / `localhost` (host)
- **Port:** 5432
- **Database:** `bpo_intel`
- **User:** `postgres`
- **Password:** `ops/secrets/postgres_password.txt`

### Schema
- **Reference:** `ops/schema.sql`
- **Init Script:** `ops/init-scripts/01_init.sql`
- **Migration:** `alembic/versions/001_initial_schema.py`

### Tables
- `documents` - Document metadata
- `document_chunks` - Text chunks
- `entities` - Extracted entities
- `relationships` - Entity relationships
- `document_embeddings` - Vector embeddings (pgvector)

---

## Docker Volumes

### Canonical Mount Points (docker-compose.yml)
```yaml
volumes:
  - "./data:/data"
  - "./Heuristics:/heuristics"
  - "./src:/app/src"
  - "./ops/init-scripts:/docker-entrypoint-initdb.d"
```

**Inside containers:**
- Raw dataset: `/data/raw/dataset_webscrape-bpo_2025-10-13_10-15-17-310 (1).json`
- Preprocessed: `/data/processed/preprocessed.jsonl`
- Heuristics: `/heuristics/*.json`

---

## Environment Variables

### Key Paths
```bash
# Database
DB_HOST=postgres  # or localhost from host
DB_PORT=5432
DB_NAME=bpo_intel
DB_USER=postgres
DB_PASSWORD_FILE=/run/secrets/postgres_password

# Data
DATA_DIR=/data
HEURISTICS_DIR=/heuristics

# Prefect
PREFECT_API_URL=http://prefect-server:4200/api

# Heuristics
HEURISTICS_VERSION=2.0.0
```

---

## Quick Reference Commands

### Preprocessing
```bash
# Generate canonical preprocessed dataset
python scripts/preprocess.py \
  --input "data/raw/dataset_webscrape-bpo_2025-10-13_10-15-17-310 (1).json" \
  --output "data/processed/preprocessed.jsonl"
```

### Extraction (Production)
```bash
# Option 1: With Prefect
python queue_extraction_prefect.py

# Option 2: Direct
python run_direct_extraction.py

# Monitor Prefect
open http://localhost:4200
```

### Validation
```bash
# Label Studio
open http://localhost:8082
```

### Docker
```bash
# Start services
docker-compose --profile base up -d

# Check logs
docker logs bpo-prefect-agent
docker logs bpo-api
```

---

## Verification

### Check Dataset
```bash
# Raw dataset exists
ls -lh "data/raw/dataset_webscrape-bpo_2025-10-13_10-15-17-310 (1).json"

# Preprocessed dataset exists
ls -lh "data/processed/preprocessed.jsonl"

# Count lines (documents)
wc -l "data/processed/preprocessed.jsonl"
```

### Check Database
```bash
# Connect to database
docker exec -it bpo-postgres psql -U postgres -d bpo_intel

# Check entity count
SELECT COUNT(*) FROM entities;

# Check relationship count
SELECT COUNT(*) FROM relationships;
```

---

## Migration History

### October 31, 2025
- ✅ Established canonical dataset paths
- ✅ Archived deprecated scripts
- ✅ Created this documentation
- ✅ Updated all extraction scripts to use `data/processed/preprocessed.jsonl`

### October 25, 2025
- ✅ Migrated from Temporal to Prefect orchestration
- ✅ Simplified deployment and execution

### October 13, 2025
- ✅ Raw dataset collected via web scraping (45,403 documents)

---

**For Questions:** See `EXTRACTION_FIXES_SUMMARY.md` or `memory-bank/activeContext.md`

